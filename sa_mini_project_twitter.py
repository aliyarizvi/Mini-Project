# -*- coding: utf-8 -*-
"""SA_Mini_Project_Twitter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18AFZ65yfEOKWCQz-OFf_17NcF_eFfC12
"""

import numpy as np
import pandas as pd
import matplotlib
import matplotlib.pyplot as plt

path = "https://raw.githubusercontent.com/devvratmiglani/SA_Mini_Project_5th_SEM/main/Mental-Health-Twitter-Cleaned.csv"
df = pd.read_csv(path)

"""**Data Preprocessing**"""

df.head()

df.info()

df

# checking missing values
df.isnull().sum()

df = df.dropna()

# rechecking missing values
df.isnull().sum()

# checking for duplicate values
df[df.duplicated()]

df = df.drop_duplicates()

df

category_counts = df['label'].value_counts()

# Bar chart
plt.figure(figsize=(6, 4))
category_counts.plot(kind='bar')
plt.xlabel('Label')
plt.ylabel('Counts')
plt.show()
print()

plt.show()

# it is a method by which we can turn the words into their base form ie simpler form which eventually helps us to perform and train our model well

import nltk
nltk.download('wordnet')
nltk.download('punkt')

from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

lemmatizer = WordNetLemmatizer()

def text_lemmatize(text):
    word_list = nltk.word_tokenize(text) # tokenize the sentence

    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])  # Lemmatize list of words and join

    return lemmatized_output

df['post_text'] = df['post_text'].apply(text_lemmatize)

df.head(5)

# removal of stop words

def remove_stopwords(sentence):
    stopwords = ["a", "about", "above", "after", "again", "against", "all", "am", "an", "and", "any", "are", "as", "at", "be", "because", "been", "before", "being", "below", "between", "both", "but", "by", "could", "did", "do", "does", "doing", "down", "during", "each", "few", "for", "from", "further", "had", "has", "have", "having",
                 "he", "he'd", "he'll", "he's", "her", "here", "here's", "hers", "herself", "him", "himself", "his", "how", "how's", "i", "i'd", "i'll", "i'm", "i've", "if", "in", "into", "is", "it", "it's", "its", "itself", "let's", "me", "more", "most", "my", "myself", "nor", "of", "on", "once", "only", "or", "other", "ought", "our", "ours", "ourselves", "out",
                 "over", "own", "same", "she", "she'd", "she'll", "she's", "should", "so", "some", "such", "than", "that", "that's", "the", "their", "theirs", "them", "themselves", "then", "there", "there's", "these", "they", "they'd", "they'll", "they're", "they've", "this", "those", "through", "to", "too", "under", "until", "up", "very", "was",
                 "we", "we'd", "we'll", "we're", "we've", "were", "what", "what's", "when", "when's", "where", "where's", "which", "while", "who", "who's", "whom", "why", "why's", "with", "would", "you", "you'd", "you'll", "you're", "you've", "your", "yours", "yourself", "yourselves" ]

    sentence = sentence.lower()

    words = sentence.split()
    no_words = [w for w in words if w not in stopwords]
    sentence = " ".join(no_words)

    return sentence

df['post_text'] = df['post_text'].apply(remove_stopwords)

df['label_name'] = df.label.map({
    0: 'No stress',
    1: 'Stress'
})

df.head(-5)

# splitting the data into train and test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df.post_text,
    df.label,
    test_size=0.2, # 20% samples will go to test dataset ie 80-20
    random_state=42,
    stratify=df.label
)

print("Shape of X_train: ", X_train.shape)
print("Shape of X_test: ", X_test.shape)

X_train.head()

# we will be using tf-idf for text vectorization which is simply converting the text into mumeric vectors

#1. Multinomial Naive Bayes Classifier

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report

classification_reports = {}

mnb_model = Pipeline([
     ('vectorizer_tfidf',TfidfVectorizer()),
     ('MultinomialNB', MultinomialNB())
])

mnb_model.fit(X_train, y_train)

y_pred = mnb_model.predict(X_test)

CR = classification_report(y_test, y_pred)
classification_reports['Multinomial Naive Bayes Classifier'] = CR
print(CR)

# Multinomial Naive Bayes has the accuracy of 85%

X_test[:5]

y_test[:5]

y_pred[:5]

#2. Random Forest Classifier

from sklearn.ensemble import RandomForestClassifier

rf_model = Pipeline([
     ('vectorizer_tfidf',TfidfVectorizer()),
     ('Random Forest', RandomForestClassifier())
])

rf_model.fit(X_train, y_train)

y_pred = rf_model.predict(X_test)

CR = classification_report(y_test, y_pred)
classification_reports['Random Forest Classifier'] = CR
print(CR)

# Random Forest Classifier has the accuracy of 80%

#3. Support Vector Machine

from sklearn.svm import SVC

svm_model = Pipeline([
     ('vectorizer_tfidf',TfidfVectorizer()),
     ('SVM', SVC(probability=True))
])

svm_model.fit(X_train, y_train)

y_pred = svm_model.predict(X_test)

CR = classification_report(y_test, y_pred)
classification_reports['Support Vector Machine'] = CR
print(CR)

# Support Vector Machine has the accuracy of 85%

# 4. Decision Tree Classifier

from sklearn.tree import DecisionTreeClassifier

dt_model = Pipeline([
     ('vectorizer_tfidf',TfidfVectorizer()),
     ('Decision Tree', DecisionTreeClassifier())
])

dt_model.fit(X_train, y_train)

y_pred = dt_model.predict(X_test)

CR = classification_report(y_test, y_pred)
classification_reports['Decision Tree Classifier'] = CR
print(CR)

# Decision Tree has the accuracy of 76%

# Final Ensemble Classifier
# We'll be using the ensemble voting classifier and the soft voting method. As the accuracy of Random Forest classifier and SVM is seen to be better than Decision Tree and Multinomial
## Naive Bayes, we'll be using them as the base classifiers in the ensemble classifiers

from sklearn.ensemble import VotingClassifier

voting_classifier = VotingClassifier(
    estimators=[
        ('SVM', svm_model),
        ('Multinomial Naive Bayes', mnb_model)
    ],
    voting='soft'  # Use soft voting for probability-based weighting
)

voting_classifier.fit(X_train, y_train)
y_pred = voting_classifier.predict(X_test)

CR = classification_report(y_test, y_pred)
classification_reports['Final Ensemble Classifier'] = CR
print(CR)

#Final Ensemble accuracy is 86%

models = ['Multinomial Naive Bayes', 'Random Forest Classifier', 'Support Vector Machine', 'Decision Tree Classifier','Final Ensemble Model']
accuracy_values = [86, 80, 86, 76, 87]
bars = plt.bar(models, accuracy_values)

plt.ylabel('Accuracy')
plt.title('Accuracy of Machine Learning Models')
plt.xticks(rotation=20)

for bar, value in zip(bars, accuracy_values):
    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{value:.2f}',
             ha='center', va='bottom')

plt.show()

for key, value in classification_reports.items():
    print(f'-------------------------{key}-------------------------')
    print(f'{value}\n')

    